{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "import cv2 as cv\n",
    "import os\n",
    "import DarkArtefactRemoval as dca\n",
    "import dullrazor as dr\n",
    "import segmentation_and_preprocessing as sp\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import gradcam_test as gc\n",
    "import vanilla_backprop as vb\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Répertoire contenant les images\n",
    "image_dir_train = 'Train/Train/'\n",
    "image_dir_test = 'Test/Test/'\n",
    "\n",
    "Train_path = image_dir_train\n",
    "\n",
    "import glob\n",
    "images_train = glob.glob(Train_path + '/*[0-9].jpg')\n",
    "mask_img_train = glob.glob(Train_path + '/*seg.png')\n",
    "\n",
    "images_with_mask = [ Train_path + mask_img_train[i].split('/')[-1].split('_seg')[0] + '.jpg' for i in range(len(mask_img_train))]\n",
    "images_test = glob.glob(image_dir_test + '/*[0-9].jpg')\n",
    "mask_img_test = glob.glob(image_dir_test + '/*seg.png')\n",
    "\n",
    "#Lire le csv metadataTrain et metadataTest\n",
    "metadataTrain = pd.read_csv('metadataTrain.csv')\n",
    "metadataTest = pd.read_csv('metadataTest.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lesions_segmentes = glob.glob('output_masks_train_set_1/*.png')\n",
    "lesions_segmentes_names = [os.path.basename(x).split('.jpg_pred_mask.png')[0] for x in lesions_segmentes]\n",
    "\n",
    "binary_masks = glob.glob('output_binary_masks_train_set_1/*.png')\n",
    "binary_masks_names = [os.path.basename(x).split('.jpg_pred_mask.png')[0] for x in binary_masks]\n",
    "\n",
    "# Get the names and classes as pandas Series\n",
    "names_series = metadataTrain[\"ID\"].loc[metadataTrain[\"ID\"].isin(lesions_segmentes_names)]\n",
    "classes_series = metadataTrain[\"CLASS\"].loc[metadataTrain[\"ID\"].isin(lesions_segmentes_names)]\n",
    "\n",
    "# Convert the pandas Series to lists\n",
    "names_list = names_series.tolist()\n",
    "classes_list = classes_series.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18998 train images\n",
      "There are 1945 train images with mask\n",
      "There are 1945 train masks\n",
      "There are 6333 test images\n",
      "There are 648 test masks\n",
      "There are 18998 segmented lesions\n",
      "There are 18998 binary masks\n"
     ]
    }
   ],
   "source": [
    "# Checking if the number of images is right \n",
    "print('There are', len(images_train),  'train images')\n",
    "print('There are', len(images_with_mask),  'train images with mask')\n",
    "print('There are', len(mask_img_train),  'train masks')\n",
    "print('There are', len(images_test),  'test images')\n",
    "print('There are', len(mask_img_test),  'test masks')\n",
    "print('There are', len(lesions_segmentes),  'segmented lesions')\n",
    "print('There are', len(binary_masks),  'binary masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réordonner les images\n",
    "lesions_segmentes_ordered = []\n",
    "binary_masks_ordered = []\n",
    "lesions_segmentes_ordered_names = []\n",
    "binary_masks_ordered_names = []\n",
    "images_train_ordered = []\n",
    "for i in range(0, len(names_list)):\n",
    "    for j in range(0, len(lesions_segmentes_names)):\n",
    "        if names_list[i] in lesions_segmentes_names[j]:\n",
    "            lesions_segmentes_ordered.append(lesions_segmentes[j])\n",
    "            binary_masks_ordered.append(binary_masks[j])\n",
    "            images_train_ordered.append(images_train[i])\n",
    "            lesions_segmentes_ordered_names.append(lesions_segmentes_names[j])\n",
    "            binary_masks_ordered_names.append(binary_masks_names[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the segmentation mask for all the 20 000 images of the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation des données\n",
    "\n",
    "- Images carrées de 256x256\n",
    "- Normalisation des images\n",
    "- Augmentation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18998\n",
      "18998\n",
      "['output_masks_train_set_1/ISIC_0028766.jpg_pred_mask.png'\n",
      " 'output_masks_train_set_1/ISIC_0071222.jpg_pred_mask.png'\n",
      " 'output_masks_train_set_1/ISIC_0069434.jpg_pred_mask.png' ...\n",
      " 'output_masks_train_set_1/ISIC_0030034.jpg_pred_mask.png'\n",
      " 'output_masks_train_set_1/ISIC_0027095.jpg_pred_mask.png'\n",
      " 'output_masks_train_set_1/ISIC_0031967.jpg_pred_mask.png']\n",
      "[2 8 3 ... 1 2 5]\n",
      "                                                      ID   CLASS\n",
      "0      output_masks_train_set_1/ISIC_0012711.jpg_pred...  CLASS2\n",
      "1      output_masks_train_set_1/ISIC_0067801.jpg_pred...  CLASS2\n",
      "2      output_masks_train_set_1/ISIC_0026869.jpg_pred...  CLASS2\n",
      "3      output_masks_train_set_1/ISIC_0069630.jpg_pred...  CLASS4\n",
      "4      output_masks_train_set_1/ISIC_0032969.jpg_pred...  CLASS2\n",
      "...                                                  ...     ...\n",
      "15193  output_masks_train_set_1/ISIC_0028785.jpg_pred...  CLASS2\n",
      "15194  output_masks_train_set_1/ISIC_0063294.jpg_pred...  CLASS2\n",
      "15195  output_masks_train_set_1/ISIC_0058228.jpg_pred...  CLASS2\n",
      "15196  output_masks_train_set_1/ISIC_0064156.jpg_pred...  CLASS3\n",
      "15197  output_masks_train_set_1/ISIC_0065028.jpg_pred...  CLASS3\n",
      "\n",
      "[15198 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "X_train = lesions_segmentes_ordered\n",
    "X_train = np.array(X_train)\n",
    "y_train = classes_list\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "#----------------------------------\n",
    "#y_train = y_train - 1\n",
    "#----------------------------------\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "#Créer la validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=50)\n",
    "\n",
    "#créer le csv avec la correspondance nom - class\n",
    "with open('X_train_ID_class.csv', mode='w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['ID', 'CLASS'])\n",
    "    for i in range(len(X_train)):\n",
    "        writer.writerow([X_train[i], y_train[i]])\n",
    "\n",
    "with open('X_val_ID_class.csv', mode='w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['ID', 'CLASS'])\n",
    "    for i in range(len(X_val)):\n",
    "        writer.writerow([X_val[i], y_val[i]])\n",
    "\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('X_train_ID_class.csv')\n",
    "df_val = pd.read_csv('X_val_ID_class.csv')\n",
    "df_train['CLASS'] = df_train['CLASS'].astype(str)\n",
    "df_val['CLASS'] = df_val['CLASS'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(df_train)):\n",
    "    df_train['CLASS'][i] = df_train['CLASS'][i].replace('1', 'CLASS1')\n",
    "    df_train['CLASS'][i] = df_train['CLASS'][i].replace('2', 'CLASS2')\n",
    "    df_train['CLASS'][i] = df_train['CLASS'][i].replace('3', 'CLASS3')\n",
    "    df_train['CLASS'][i] = df_train['CLASS'][i].replace('4', 'CLASS4')\n",
    "    df_train['CLASS'][i] = df_train['CLASS'][i].replace('5', 'CLASS5')\n",
    "    df_train['CLASS'][i] = df_train['CLASS'][i].replace('6', 'CLASS6')\n",
    "    df_train['CLASS'][i] = df_train['CLASS'][i].replace('7', 'CLASS7')\n",
    "    df_train['CLASS'][i] = df_train['CLASS'][i].replace('8', 'CLASS8')\n",
    "print(df_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2758., 7707., 1975.,    0.,  529., 1583.,    0.,  140.,  140.,\n",
       "         366.]),\n",
       " array([1. , 1.7, 2.4, 3.1, 3.8, 4.5, 5.2, 5.9, 6.6, 7.3, 8. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjEAAAGeCAYAAABlzVBSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAuhElEQVR4nO3df3RU9Z3/8deYkABpcksCM8Mcg8Q2RSDRusETEm2hy+8SYo89gkZnUSjggsAUKD9kd4seTYDdgt3NWQTWA8iPjX+0WK0agW6NZSEQomkBEe2KGDRDaDtMgsYJhvv9wy93dwhFJkiGT3g+zvn8Mfe+7533J8YzLz5z743Ltm1bAAAAhrkh3g0AAAB0BCEGAAAYiRADAACMRIgBAABGIsQAAAAjEWIAAICRCDEAAMBIhBgAAGAkQgwAADBSYrwbuFrOnTunjz/+WKmpqXK5XPFuBwAAXAbbttXc3Cyfz6cbbviStRY7BmfPnrWXLl1q9+/f3+7evbudlZVlP/7443ZbW5tTc+7cOfunP/2p3bdvX7t79+72sGHD7EOHDkWd57PPPrMfffRROyMjw+7Zs6c9YcIEu76+PqrmL3/5i/3ggw/aaWlpdlpamv3ggw/aoVDosnutr6+3JTEYDAaDwTBwXJgLLiamlZgVK1bomWee0aZNmzR48GAdOHBADz/8sCzL0ty5cyVJK1eu1KpVq7Rx40Z961vf0pNPPqlRo0bp6NGjSk1NlSQFAgG99NJLqqioUEZGhubPn6+ioiLV1tYqISFBklRSUqITJ06osrJSkjR9+nT5/X699NJLl9Xr+feqr69XWlpaLNMEAABx0tTUpMzMTOdz/JIue2nDtu3x48fbU6ZMidp2zz332A8++KBt21+swni9Xnv58uXO/s8++8y2LMt+5plnbNu27dOnT9vdunWzKyoqnJqPPvrIvuGGG+zKykrbtm377bfftiXZ1dXVTs3evXttSfY777xzWb2Gw2Fbkh0Oh2OZIgAAiKNYPr9jurD3rrvu0m9+8xu9++67kqTf//732r17t77//e9Lko4dO6ZgMKjRo0c7xyQnJ2vYsGHas2ePJKm2tlZnz56NqvH5fMrJyXFq9u7dK8uylJ+f79QMHTpUlmU5NReKRCJqamqKGgAAoOuK6eukRYsWKRwO65ZbblFCQoLa2tr01FNP6f7775ckBYNBSZLH44k6zuPx6Pjx405NUlKSevXq1a7m/PHBYFBut7vd+7vdbqfmQmVlZXr88cdjmQ4AADBYTCsxzz//vLZs2aJt27bpzTff1KZNm/Qv//Iv2rRpU1TdhXcD2bb9pXcIXVhzsfpLnWfJkiUKh8POqK+vv9xpAQAAA8W0EvOTn/xEixcv1n333SdJys3N1fHjx1VWVqbJkyfL6/VK+mIlpW/fvs5xjY2NzuqM1+tVa2urQqFQ1GpMY2OjCgsLnZqTJ0+2e/9Tp061W+U5Lzk5WcnJybFMBwAAGCymlZhPP/203T3bCQkJOnfunCQpKytLXq9XO3fudPa3traqqqrKCSh5eXnq1q1bVE1DQ4MOHTrk1BQUFCgcDmv//v1Ozb59+xQOh50aAABwfYtpJWbChAl66qmn1K9fPw0ePFhvvfWWVq1apSlTpkj64iugQCCg0tJSZWdnKzs7W6WlperZs6dKSkokSZZlaerUqZo/f74yMjKUnp6uBQsWKDc3VyNHjpQkDRw4UGPHjtW0adO0du1aSV/cYl1UVKQBAwZ8lfMHAACGiinE/Nu//Zv+8R//UTNnzlRjY6N8Pp9mzJihf/qnf3JqFi5cqJaWFs2cOVOhUEj5+fnasWNH1P3eq1evVmJioiZOnKiWlhaNGDFCGzdudJ4RI0lbt27VnDlznLuYiouLVV5efqXzBQAAXYTLtm073k1cDU1NTbIsS+FwmIfdAQBgiFg+v/kDkAAAwEiEGAAAYCRCDAAAMBIhBgAAGIkQAwAAjESIAQAARorpOTEwW//FL8e7hZh9sHx8vFsAAFyjWIkBAABGIsQAAAAjEWIAAICRCDEAAMBIhBgAAGAkQgwAADASIQYAABiJEAMAAIxEiAEAAEYixAAAACMRYgAAgJEIMQAAwEiEGAAAYCRCDAAAMBIhBgAAGIkQAwAAjESIAQAARiLEAAAAIxFiAACAkQgxAADASIQYAABgJEIMAAAwEiEGAAAYiRADAACMRIgBAABGIsQAAAAjEWIAAICRCDEAAMBIMYWY/v37y+VytRuzZs2SJNm2rWXLlsnn86lHjx4aPny4Dh8+HHWOSCSi2bNnq3fv3kpJSVFxcbFOnDgRVRMKheT3+2VZlizLkt/v1+nTp69spgAAoEuJKcTU1NSooaHBGTt37pQk3XvvvZKklStXatWqVSovL1dNTY28Xq9GjRql5uZm5xyBQEDbt29XRUWFdu/erTNnzqioqEhtbW1OTUlJierq6lRZWanKykrV1dXJ7/d/FfMFAABdhMu2bbujBwcCAf3617/We++9J0ny+XwKBAJatGiRpC9WXTwej1asWKEZM2YoHA6rT58+2rx5syZNmiRJ+vjjj5WZmalXXnlFY8aM0ZEjRzRo0CBVV1crPz9fklRdXa2CggK98847GjBgwGX11tTUJMuyFA6HlZaW1tEpdin9F78c7xZi9sHy8fFuAQDQiWL5/O7wNTGtra3asmWLpkyZIpfLpWPHjikYDGr06NFOTXJysoYNG6Y9e/ZIkmpra3X27NmoGp/Pp5ycHKdm7969sizLCTCSNHToUFmW5dRcTCQSUVNTU9QAAABdV4dDzAsvvKDTp0/roYcekiQFg0FJksfjiarzeDzOvmAwqKSkJPXq1euSNW63u937ud1up+ZiysrKnGtoLMtSZmZmR6cGAAAM0OEQ8+yzz2rcuHHy+XxR210uV9Rr27bbbbvQhTUXq/+y8yxZskThcNgZ9fX1lzMNAABgqA6FmOPHj2vXrl360Y9+5Gzzer2S1G61pLGx0Vmd8Xq9am1tVSgUumTNyZMn273nqVOn2q3y/F/JyclKS0uLGgAAoOvqUIjZsGGD3G63xo//34sus7Ky5PV6nTuWpC+um6mqqlJhYaEkKS8vT926dYuqaWho0KFDh5yagoIChcNh7d+/36nZt2+fwuGwUwMAAJAY6wHnzp3Thg0bNHnyZCUm/u/hLpdLgUBApaWlys7OVnZ2tkpLS9WzZ0+VlJRIkizL0tSpUzV//nxlZGQoPT1dCxYsUG5urkaOHClJGjhwoMaOHatp06Zp7dq1kqTp06erqKjosu9MAgAAXV/MIWbXrl368MMPNWXKlHb7Fi5cqJaWFs2cOVOhUEj5+fnasWOHUlNTnZrVq1crMTFREydOVEtLi0aMGKGNGzcqISHBqdm6davmzJnj3MVUXFys8vLyjswPAAB0UVf0nJhrGc+JaY/nxAAArnWd8pwYAACAeCLEAAAAIxFiAACAkQgxAADASIQYAABgJEIMAAAwEiEGAAAYiRADAACMRIgBAABGIsQAAAAjEWIAAICRCDEAAMBIhBgAAGAkQgwAADASIQYAABiJEAMAAIxEiAEAAEYixAAAACMRYgAAgJEIMQAAwEiEGAAAYCRCDAAAMBIhBgAAGIkQAwAAjESIAQAARiLEAAAAIxFiAACAkQgxAADASIQYAABgJEIMAAAwEiEGAAAYiRADAACMRIgBAABGIsQAAAAjxRxiPvroIz344IPKyMhQz5499e1vf1u1tbXOftu2tWzZMvl8PvXo0UPDhw/X4cOHo84RiUQ0e/Zs9e7dWykpKSouLtaJEyeiakKhkPx+vyzLkmVZ8vv9On36dMdmCQAAupyYQkwoFNKdd96pbt266dVXX9Xbb7+tn/3sZ/r617/u1KxcuVKrVq1SeXm5ampq5PV6NWrUKDU3Nzs1gUBA27dvV0VFhXbv3q0zZ86oqKhIbW1tTk1JSYnq6upUWVmpyspK1dXVye/3X/mMAQBAl+Cybdu+3OLFixfrv//7v/W73/3uovtt25bP51MgENCiRYskfbHq4vF4tGLFCs2YMUPhcFh9+vTR5s2bNWnSJEnSxx9/rMzMTL3yyisaM2aMjhw5okGDBqm6ulr5+fmSpOrqahUUFOidd97RgAEDvrTXpqYmWZalcDistLS0y51il9Z/8cvxbiFmHywfH+8WAACdKJbP75hWYl588UUNGTJE9957r9xut26//XatX7/e2X/s2DEFg0GNHj3a2ZacnKxhw4Zpz549kqTa2lqdPXs2qsbn8yknJ8ep2bt3ryzLcgKMJA0dOlSWZTk1F4pEImpqaooaAACg64opxLz//vtas2aNsrOz9dprr+mRRx7RnDlz9Nxzz0mSgsGgJMnj8UQd5/F4nH3BYFBJSUnq1avXJWvcbne793e73U7NhcrKypzrZyzLUmZmZixTAwAAhokpxJw7d05/8zd/o9LSUt1+++2aMWOGpk2bpjVr1kTVuVyuqNe2bbfbdqELay5Wf6nzLFmyROFw2Bn19fWXOy0AAGCgmEJM3759NWjQoKhtAwcO1IcffihJ8nq9ktRutaSxsdFZnfF6vWptbVUoFLpkzcmTJ9u9/6lTp9qt8pyXnJystLS0qAEAALqumELMnXfeqaNHj0Zte/fdd3XTTTdJkrKysuT1erVz505nf2trq6qqqlRYWChJysvLU7du3aJqGhoadOjQIaemoKBA4XBY+/fvd2r27duncDjs1AAAgOtbYizFP/7xj1VYWKjS0lJNnDhR+/fv17p167Ru3TpJX3wFFAgEVFpaquzsbGVnZ6u0tFQ9e/ZUSUmJJMmyLE2dOlXz589XRkaG0tPTtWDBAuXm5mrkyJGSvljdGTt2rKZNm6a1a9dKkqZPn66ioqLLujMJAAB0fTGFmDvuuEPbt2/XkiVL9MQTTygrK0tPP/20HnjgAadm4cKFamlp0cyZMxUKhZSfn68dO3YoNTXVqVm9erUSExM1ceJEtbS0aMSIEdq4caMSEhKcmq1bt2rOnDnOXUzFxcUqLy+/0vkCAIAuIqbnxJiE58S0x3NiAADXuqv2nBgAAIBrBSEGAAAYiRADAACMRIgBAABGIsQAAAAjEWIAAICRCDEAAMBIhBgAAGAkQgwAADASIQYAABiJEAMAAIxEiAEAAEYixAAAACMRYgAAgJEIMQAAwEiEGAAAYCRCDAAAMBIhBgAAGIkQAwAAjESIAQAARiLEAAAAIxFiAACAkQgxAADASIQYAABgJEIMAAAwEiEGAAAYiRADAACMRIgBAABGIsQAAAAjEWIAAICRCDEAAMBIhBgAAGAkQgwAADASIQYAABiJEAMAAIwUU4hZtmyZXC5X1PB6vc5+27a1bNky+Xw+9ejRQ8OHD9fhw4ejzhGJRDR79mz17t1bKSkpKi4u1okTJ6JqQqGQ/H6/LMuSZVny+/06ffp0x2cJAAC6nJhXYgYPHqyGhgZnHDx40Nm3cuVKrVq1SuXl5aqpqZHX69WoUaPU3Nzs1AQCAW3fvl0VFRXavXu3zpw5o6KiIrW1tTk1JSUlqqurU2VlpSorK1VXVye/33+FUwUAAF1JYswHJCZGrb6cZ9u2nn76aS1dulT33HOPJGnTpk3yeDzatm2bZsyYoXA4rGeffVabN2/WyJEjJUlbtmxRZmamdu3apTFjxujIkSOqrKxUdXW18vPzJUnr169XQUGBjh49qgEDBlzJfAEAQBcR80rMe++9J5/Pp6ysLN133316//33JUnHjh1TMBjU6NGjndrk5GQNGzZMe/bskSTV1tbq7NmzUTU+n085OTlOzd69e2VZlhNgJGno0KGyLMupuZhIJKKmpqaoAQAAuq6YQkx+fr6ee+45vfbaa1q/fr2CwaAKCwv15z//WcFgUJLk8XiijvF4PM6+YDCopKQk9erV65I1bre73Xu73W6n5mLKysqca2gsy1JmZmYsUwMAAIaJKcSMGzdOP/zhD5Wbm6uRI0fq5ZdflvTF10bnuVyuqGNs22637UIX1lys/svOs2TJEoXDYWfU19df1pwAAICZrugW65SUFOXm5uq9995zrpO5cLWksbHRWZ3xer1qbW1VKBS6ZM3JkyfbvdepU6farfL8X8nJyUpLS4saAACg67qiEBOJRHTkyBH17dtXWVlZ8nq92rlzp7O/tbVVVVVVKiwslCTl5eWpW7duUTUNDQ06dOiQU1NQUKBwOKz9+/c7Nfv27VM4HHZqAAAAYro7acGCBZowYYL69eunxsZGPfnkk2pqatLkyZPlcrkUCARUWlqq7OxsZWdnq7S0VD179lRJSYkkybIsTZ06VfPnz1dGRobS09O1YMEC5+spSRo4cKDGjh2radOmae3atZKk6dOnq6ioiDuTAACAI6YQc+LECd1///3605/+pD59+mjo0KGqrq7WTTfdJElauHChWlpaNHPmTIVCIeXn52vHjh1KTU11zrF69WolJiZq4sSJamlp0YgRI7Rx40YlJCQ4NVu3btWcOXOcu5iKi4tVXl7+VcwXAAB0ES7btu14N3E1NDU1ybIshcNhro/5//ovfjneLcTsg+Xj490CAKATxfL5zd9OAgAARiLEAAAAIxFiAACAkQgxAADASIQYAABgJEIMAAAwEiEGAAAYiRADAACMRIgBAABGIsQAAAAjEWIAAICRCDEAAMBIhBgAAGAkQgwAADASIQYAABiJEAMAAIxEiAEAAEYixAAAACMRYgAAgJEIMQAAwEiEGAAAYCRCDAAAMBIhBgAAGIkQAwAAjESIAQAARiLEAAAAIxFiAACAkQgxAADASIQYAABgJEIMAAAwEiEGAAAYiRADAACMRIgBAABGIsQAAAAjEWIAAICRrijElJWVyeVyKRAIONts29ayZcvk8/nUo0cPDR8+XIcPH446LhKJaPbs2erdu7dSUlJUXFysEydORNWEQiH5/X5ZliXLsuT3+3X69OkraRcAAHQhHQ4xNTU1WrdunW699dao7StXrtSqVatUXl6umpoaeb1ejRo1Ss3NzU5NIBDQ9u3bVVFRod27d+vMmTMqKipSW1ubU1NSUqK6ujpVVlaqsrJSdXV18vv9HW0XAAB0MR0KMWfOnNEDDzyg9evXq1evXs5227b19NNPa+nSpbrnnnuUk5OjTZs26dNPP9W2bdskSeFwWM8++6x+9rOfaeTIkbr99tu1ZcsWHTx4ULt27ZIkHTlyRJWVlfqP//gPFRQUqKCgQOvXr9evf/1rHT169CuYNgAAMF2HQsysWbM0fvx4jRw5Mmr7sWPHFAwGNXr0aGdbcnKyhg0bpj179kiSamtrdfbs2agan8+nnJwcp2bv3r2yLEv5+flOzdChQ2VZllNzoUgkoqampqgBAAC6rsRYD6ioqNCbb76pmpqadvuCwaAkyePxRG33eDw6fvy4U5OUlBS1gnO+5vzxwWBQbre73fndbrdTc6GysjI9/vjjsU4HAAAYKqaVmPr6es2dO1dbtmxR9+7d/2qdy+WKem3bdrttF7qw5mL1lzrPkiVLFA6HnVFfX3/J9wMAAGaLKcTU1taqsbFReXl5SkxMVGJioqqqqvSv//qvSkxMdFZgLlwtaWxsdPZ5vV61trYqFApdsubkyZPt3v/UqVPtVnnOS05OVlpaWtQAAABdV0whZsSIETp48KDq6uqcMWTIED3wwAOqq6vTzTffLK/Xq507dzrHtLa2qqqqSoWFhZKkvLw8devWLaqmoaFBhw4dcmoKCgoUDoe1f/9+p2bfvn0Kh8NODQAAuL7FdE1MamqqcnJyoralpKQoIyPD2R4IBFRaWqrs7GxlZ2ertLRUPXv2VElJiSTJsixNnTpV8+fPV0ZGhtLT07VgwQLl5uY6FwoPHDhQY8eO1bRp07R27VpJ0vTp01VUVKQBAwZc8aQBAID5Yr6w98ssXLhQLS0tmjlzpkKhkPLz87Vjxw6lpqY6NatXr1ZiYqImTpyolpYWjRgxQhs3blRCQoJTs3XrVs2ZM8e5i6m4uFjl5eVfdbsAAMBQLtu27Xg3cTU0NTXJsiyFw2Guj/n/+i9+Od4txOyD5ePj3QIAoBPF8vnN304CAABGIsQAAAAjEWIAAICRCDEAAMBIhBgAAGAkQgwAADASIQYAABiJEAMAAIxEiAEAAEYixAAAACMRYgAAgJEIMQAAwEiEGAAAYCRCDAAAMBIhBgAAGIkQAwAAjESIAQAARiLEAAAAIxFiAACAkQgxAADASIQYAABgJEIMAAAwEiEGAAAYiRADAACMRIgBAABGIsQAAAAjEWIAAICRCDEAAMBIhBgAAGAkQgwAADASIQYAABiJEAMAAIxEiAEAAEYixAAAACPFFGLWrFmjW2+9VWlpaUpLS1NBQYFeffVVZ79t21q2bJl8Pp969Oih4cOH6/Dhw1HniEQimj17tnr37q2UlBQVFxfrxIkTUTWhUEh+v1+WZcmyLPn9fp0+fbrjswQAAF1OTCHmxhtv1PLly3XgwAEdOHBAf/u3f6u7777bCSorV67UqlWrVF5erpqaGnm9Xo0aNUrNzc3OOQKBgLZv366Kigrt3r1bZ86cUVFRkdra2pyakpIS1dXVqbKyUpWVlaqrq5Pf7/+KpgwAALoCl23b9pWcID09Xf/8z/+sKVOmyOfzKRAIaNGiRZK+WHXxeDxasWKFZsyYoXA4rD59+mjz5s2aNGmSJOnjjz9WZmamXnnlFY0ZM0ZHjhzRoEGDVF1drfz8fElSdXW1CgoK9M4772jAgAGX1VdTU5Msy1I4HFZaWtqVTLHL6L/45Xi3ELMPlo+PdwsAgE4Uy+d3h6+JaWtrU0VFhT755BMVFBTo2LFjCgaDGj16tFOTnJysYcOGac+ePZKk2tpanT17NqrG5/MpJyfHqdm7d68sy3ICjCQNHTpUlmU5NRcTiUTU1NQUNQAAQNcVc4g5ePCgvva1ryk5OVmPPPKItm/frkGDBikYDEqSPB5PVL3H43H2BYNBJSUlqVevXpescbvd7d7X7XY7NRdTVlbmXENjWZYyMzNjnRoAADBIzCFmwIABqqurU3V1tf7+7/9ekydP1ttvv+3sd7lcUfW2bbfbdqELay5W/2XnWbJkicLhsDPq6+svd0oAAMBAMYeYpKQkffOb39SQIUNUVlam2267TT//+c/l9Xolqd1qSWNjo7M64/V61draqlAodMmakydPtnvfU6dOtVvl+b+Sk5Odu6bODwAA0HVd8XNibNtWJBJRVlaWvF6vdu7c6exrbW1VVVWVCgsLJUl5eXnq1q1bVE1DQ4MOHTrk1BQUFCgcDmv//v1Ozb59+xQOh50aAACAxFiKH3vsMY0bN06ZmZlqbm5WRUWFXn/9dVVWVsrlcikQCKi0tFTZ2dnKzs5WaWmpevbsqZKSEkmSZVmaOnWq5s+fr4yMDKWnp2vBggXKzc3VyJEjJUkDBw7U2LFjNW3aNK1du1aSNH36dBUVFV32nUkAAKDriynEnDx5Un6/Xw0NDbIsS7feeqsqKys1atQoSdLChQvV0tKimTNnKhQKKT8/Xzt27FBqaqpzjtWrVysxMVETJ05US0uLRowYoY0bNyohIcGp2bp1q+bMmePcxVRcXKzy8vKvYr4AAKCLuOLnxFyreE5MezwnBgBwreuU58QAAADEEyEGAAAYiRADAACMRIgBAABGiunuJPwvEy+SBQCgK2ElBgAAGIkQAwAAjESIAQAARiLEAAAAIxFiAACAkQgxAADASIQYAABgJEIMAAAwEiEGAAAYiRADAACMRIgBAABGIsQAAAAjEWIAAICRCDEAAMBIhBgAAGAkQgwAADASIQYAABiJEAMAAIxEiAEAAEYixAAAACMRYgAAgJEIMQAAwEiEGAAAYCRCDAAAMBIhBgAAGIkQAwAAjESIAQAARiLEAAAAIxFiAACAkQgxAADASDGFmLKyMt1xxx1KTU2V2+3WD37wAx09ejSqxrZtLVu2TD6fTz169NDw4cN1+PDhqJpIJKLZs2erd+/eSklJUXFxsU6cOBFVEwqF5Pf7ZVmWLMuS3+/X6dOnOzZLAADQ5cQUYqqqqjRr1ixVV1dr586d+vzzzzV69Gh98sknTs3KlSu1atUqlZeXq6amRl6vV6NGjVJzc7NTEwgEtH37dlVUVGj37t06c+aMioqK1NbW5tSUlJSorq5OlZWVqqysVF1dnfx+/1cwZQAA0BW4bNu2O3rwqVOn5Ha7VVVVpe9+97uybVs+n0+BQECLFi2S9MWqi8fj0YoVKzRjxgyFw2H16dNHmzdv1qRJkyRJH3/8sTIzM/XKK69ozJgxOnLkiAYNGqTq6mrl5+dLkqqrq1VQUKB33nlHAwYM+NLempqaZFmWwuGw0tLSOjrFv6r/4pe/8nOivQ+Wj493CwCAThTL5/cVXRMTDoclSenp6ZKkY8eOKRgMavTo0U5NcnKyhg0bpj179kiSamtrdfbs2agan8+nnJwcp2bv3r2yLMsJMJI0dOhQWZbl1FwoEomoqakpagAAgK6rwyHGtm3NmzdPd911l3JyciRJwWBQkuTxeKJqPR6Psy8YDCopKUm9evW6ZI3b7W73nm6326m5UFlZmXP9jGVZyszM7OjUAACAARI7euCjjz6qP/zhD9q9e3e7fS6XK+q1bdvttl3owpqL1V/qPEuWLNG8efOc101NTQSZLsDEr+34CgwAOkeHVmJmz56tF198Ub/97W914403Otu9Xq8ktVstaWxsdFZnvF6vWltbFQqFLllz8uTJdu976tSpdqs85yUnJystLS1qAACAriumEGPbth599FH98pe/1H/9138pKysran9WVpa8Xq927tzpbGttbVVVVZUKCwslSXl5eerWrVtUTUNDgw4dOuTUFBQUKBwOa//+/U7Nvn37FA6HnRoAAHB9i+nrpFmzZmnbtm361a9+pdTUVGfFxbIs9ejRQy6XS4FAQKWlpcrOzlZ2drZKS0vVs2dPlZSUOLVTp07V/PnzlZGRofT0dC1YsEC5ubkaOXKkJGngwIEaO3aspk2bprVr10qSpk+frqKiosu6MwkAAHR9MYWYNWvWSJKGDx8etX3Dhg166KGHJEkLFy5US0uLZs6cqVAopPz8fO3YsUOpqalO/erVq5WYmKiJEyeqpaVFI0aM0MaNG5WQkODUbN26VXPmzHHuYiouLlZ5eXlH5ggAALqgK3pOzLWM58QgXriwFwA6rtOeEwMAABAvhBgAAGAkQgwAADASIQYAABiJEAMAAIxEiAEAAEYixAAAACMRYgAAgJEIMQAAwEiEGAAAYCRCDAAAMBIhBgAAGCmmv2INANcKE/8IK38cFPhqsRIDAACMRIgBAABGIsQAAAAjEWIAAICRCDEAAMBIhBgAAGAkQgwAADASIQYAABiJEAMAAIxEiAEAAEYixAAAACMRYgAAgJEIMQAAwEiEGAAAYCRCDAAAMBIhBgAAGIkQAwAAjESIAQAARiLEAAAAIxFiAACAkQgxAADASIQYAABgpJhDzBtvvKEJEybI5/PJ5XLphRdeiNpv27aWLVsmn8+nHj16aPjw4Tp8+HBUTSQS0ezZs9W7d2+lpKSouLhYJ06ciKoJhULy+/2yLEuWZcnv9+v06dMxTxAAAHRNMYeYTz75RLfddpvKy8svun/lypVatWqVysvLVVNTI6/Xq1GjRqm5udmpCQQC2r59uyoqKrR7926dOXNGRUVFamtrc2pKSkpUV1enyspKVVZWqq6uTn6/vwNTBAAAXVFirAeMGzdO48aNu+g+27b19NNPa+nSpbrnnnskSZs2bZLH49G2bds0Y8YMhcNhPfvss9q8ebNGjhwpSdqyZYsyMzO1a9cujRkzRkeOHFFlZaWqq6uVn58vSVq/fr0KCgp09OhRDRgwoKPzBQAAXcRXek3MsWPHFAwGNXr0aGdbcnKyhg0bpj179kiSamtrdfbs2agan8+nnJwcp2bv3r2yLMsJMJI0dOhQWZbl1FwoEomoqakpagAAgK7rKw0xwWBQkuTxeKK2ezweZ18wGFRSUpJ69ep1yRq3293u/G6326m5UFlZmXP9jGVZyszMvOL5AACAa9dVuTvJ5XJFvbZtu922C11Yc7H6S51nyZIlCofDzqivr+9A5wAAwBRfaYjxer2S1G61pLGx0Vmd8Xq9am1tVSgUumTNyZMn253/1KlT7VZ5zktOTlZaWlrUAAAAXddXGmKysrLk9Xq1c+dOZ1tra6uqqqpUWFgoScrLy1O3bt2iahoaGnTo0CGnpqCgQOFwWPv373dq9u3bp3A47NQAAIDrW8x3J505c0Z//OMfndfHjh1TXV2d0tPT1a9fPwUCAZWWlio7O1vZ2dkqLS1Vz549VVJSIkmyLEtTp07V/PnzlZGRofT0dC1YsEC5ubnO3UoDBw7U2LFjNW3aNK1du1aSNH36dBUVFXFnEgAAkNSBEHPgwAF973vfc17PmzdPkjR58mRt3LhRCxcuVEtLi2bOnKlQKKT8/Hzt2LFDqampzjGrV69WYmKiJk6cqJaWFo0YMUIbN25UQkKCU7N161bNmTPHuYupuLj4rz6bBgAAXH9ctm3b8W7iamhqapJlWQqHw1fl+pj+i1/+ys+JruGD5ePj3cJ1wcT/B/ndAL5cLJ/f/O0kAABgJEIMAAAwEiEGAAAYiRADAACMRIgBAABGIsQAAAAjEWIAAICRCDEAAMBIhBgAAGAkQgwAADASIQYAABiJEAMAAIxEiAEAAEYixAAAACMlxrsBAPHXf/HL8W4BAGLGSgwAADASIQYAABiJEAMAAIxEiAEAAEYixAAAACMRYgAAgJEIMQAAwEg8JwYAgGuAic9r+mD5+Li+PysxAADASIQYAABgJEIMAAAwEiEGAAAYiRADAACMRIgBAABGIsQAAAAjEWIAAICRCDEAAMBIPLEXAPBXmfgUWVw/WIkBAABGuuZDzL//+78rKytL3bt3V15enn73u9/FuyUAAHANuKZDzPPPP69AIKClS5fqrbfe0ne+8x2NGzdOH374YbxbAwAAcXZNh5hVq1Zp6tSp+tGPfqSBAwfq6aefVmZmptasWRPv1gAAQJxdsxf2tra2qra2VosXL47aPnr0aO3Zs6ddfSQSUSQScV6Hw2FJUlNT01Xp71zk06tyXpjvav3OXU38PncOfjfQ1VyN3+nz57Rt+0trr9kQ86c//UltbW3yeDxR2z0ej4LBYLv6srIyPf744+22Z2ZmXrUegYuxno53B7hW8buBruZq/k43NzfLsqxL1lyzIeY8l8sV9dq27XbbJGnJkiWaN2+e8/rcuXP6y1/+ooyMjIvWX4mmpiZlZmaqvr5eaWlpX+m5TXC9z1/iZ8D8r+/5S/wMrvf5S1fvZ2Dbtpqbm+Xz+b609poNMb1791ZCQkK7VZfGxsZ2qzOSlJycrOTk5KhtX//6169mi0pLS7tuf3kl5i/xM2D+1/f8JX4G1/v8pavzM/iyFZjzrtkLe5OSkpSXl6edO3dGbd+5c6cKCwvj1BUAALhWXLMrMZI0b948+f1+DRkyRAUFBVq3bp0+/PBDPfLII/FuDQAAxNk1HWImTZqkP//5z3riiSfU0NCgnJwcvfLKK7rpppvi2ldycrJ++tOftvv66npxvc9f4mfA/K/v+Uv8DK73+UvXxs/AZV/OPUwAAADXmGv2mhgAAIBLIcQAAAAjEWIAAICRCDEAAMBIhBgAAGAkQkwM3njjDU2YMEE+n08ul0svvPBCvFvqVGVlZbrjjjuUmpoqt9utH/zgBzp69Gi82+o0a9as0a233uo8nbKgoECvvvpqvNuKm7KyMrlcLgUCgXi30mmWLVsml8sVNbxeb7zb6lQfffSRHnzwQWVkZKhnz5769re/rdra2ni31Wn69+/f7nfA5XJp1qxZ8W6tU3z++ef6h3/4B2VlZalHjx66+eab9cQTT+jcuXNx6eeafk7MteaTTz7Rbbfdpocfflg//OEP491Op6uqqtKsWbN0xx136PPPP9fSpUs1evRovf3220pJSYl3e1fdjTfeqOXLl+ub3/ymJGnTpk26++679dZbb2nw4MFx7q5z1dTUaN26dbr11lvj3UqnGzx4sHbt2uW8TkhIiGM3nSsUCunOO+/U9773Pb366qtyu936n//5n6v+J16uJTU1NWpra3NeHzp0SKNGjdK9994bx646z4oVK/TMM89o06ZNGjx4sA4cOKCHH35YlmVp7ty5nd4PISYG48aN07hx4+LdRtxUVlZGvd6wYYPcbrdqa2v13e9+N05ddZ4JEyZEvX7qqae0Zs0aVVdXX1ch5syZM3rggQe0fv16Pfnkk/Fup9MlJiZed6sv561YsUKZmZnasGGDs61///7xaygO+vTpE/V6+fLl+sY3vqFhw4bFqaPOtXfvXt19990aP368pC/++//nf/6nDhw4EJd++DoJHRYOhyVJ6enpce6k87W1tamiokKffPKJCgoK4t1Op5o1a5bGjx+vkSNHxruVuHjvvffk8/mUlZWl++67T++//368W+o0L774ooYMGaJ7771Xbrdbt99+u9avXx/vtuKmtbVVW7Zs0ZQpU+RyueLdTqe466679Jvf/EbvvvuuJOn3v/+9du/ere9///tx6YeVGHSIbduaN2+e7rrrLuXk5MS7nU5z8OBBFRQU6LPPPtPXvvY1bd++XYMGDYp3W52moqJCb775pmpqauLdSlzk5+frueee07e+9S2dPHlSTz75pAoLC3X48GFlZGTEu72r7v3339eaNWs0b948PfbYY9q/f7/mzJmj5ORk/d3f/V282+t0L7zwgk6fPq2HHnoo3q10mkWLFikcDuuWW25RQkKC2tra9NRTT+n++++PSz+EGHTIo48+qj/84Q/avXt3vFvpVAMGDFBdXZ1Onz6tX/ziF5o8ebKqqqquiyBTX1+vuXPnaseOHerevXu824mL//t1cm5urgoKCvSNb3xDmzZt0rx58+LYWec4d+6chgwZotLSUknS7bffrsOHD2vNmjXXZYh59tlnNW7cOPl8vni30mmef/55bdmyRdu2bdPgwYNVV1enQCAgn8+nyZMnd3o/hBjEbPbs2XrxxRf1xhtv6MYbb4x3O50qKSnJubB3yJAhqqmp0c9//nOtXbs2zp1dfbW1tWpsbFReXp6zra2tTW+88YbKy8sViUSuq4tcJSklJUW5ubl677334t1Kp+jbt2+7wD5w4ED94he/iFNH8XP8+HHt2rVLv/zlL+PdSqf6yU9+osWLF+u+++6T9EWYP378uMrKyggxuLbZtq3Zs2dr+/btev3115WVlRXvluLOtm1FIpF4t9EpRowYoYMHD0Zte/jhh3XLLbdo0aJF112AkaRIJKIjR47oO9/5Trxb6RR33nlnu8cqvPvuu7rpppvi1FH8nL+x4fwFrteLTz/9VDfcEH05bUJCArdYm+DMmTP64x//6Lw+duyY6urqlJ6ern79+sWxs84xa9Ysbdu2Tb/61a+UmpqqYDAoSbIsSz169Ihzd1ffY489pnHjxikzM1PNzc2qqKjQ66+/3u6ura4qNTW13fVPKSkpysjIuG6ui1qwYIEmTJigfv36qbGxUU8++aSampri8i/QePjxj3+swsJClZaWauLEidq/f7/WrVundevWxbu1TnXu3Dlt2LBBkydPVmLi9fUxOmHCBD311FPq16+fBg8erLfeekurVq3SlClT4tOQjcv229/+1pbUbkyePDnerXWKi81dkr1hw4Z4t9YppkyZYt900012UlKS3adPH3vEiBH2jh074t1WXA0bNsyeO3duvNvoNJMmTbL79u1rd+vWzfb5fPY999xjHz58ON5tdaqXXnrJzsnJsZOTk+1bbrnFXrduXbxb6nSvvfaaLck+evRovFvpdE1NTfbcuXPtfv362d27d7dvvvlme+nSpXYkEolLPy7btu34xCcAAICO4zkxAADASIQYAABgJEIMAAAwEiEGAAAYiRADAACMRIgBAABGIsQAAAAjEWIAAICRCDEAAMBIhBgAAGAkQgwAADDS/wOH58i7T7JHJQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTENTION Y_train entre 1 et 8\n",
    "\n",
    "# Modification j'ajoute -1 pour le passer entre 0 et 7 pour le CNN\n",
    "\n",
    "# Ne pas oubliez de faire la démarche inverse après la prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "def AlexNet(num_classes):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # C1 Convolutional Layer\n",
    "    model.add(Conv2D(filters=96, input_shape=(227, 227, 3), kernel_size=(11, 11),\n",
    "                     strides=(4, 4), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "    # Batch Normalisation before passing it to the next layer\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # C2 Convolutional Layer\n",
    "    model.add(Conv2D(filters=256, kernel_size=(11, 11), strides=(1, 1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "    # Batch Normalization\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # C3 Convolutional Layer\n",
    "    model.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Batch Normalization\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # C4 Convolutional Layer\n",
    "    model.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Batch Normalization\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # C5 Convolutional Layer\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "    # Batch Normalization\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Flatten\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # D1 Dense Layer\n",
    "    model.add(Dense(4096, input_shape=(227 * 227 * 3,)))\n",
    "    model.add(Activation('relu'))\n",
    "    # Dropout\n",
    "    model.add(Dropout(0.4))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # D2 Dense Layer\n",
    "    model.add(Dense(4096))\n",
    "    model.add(Activation('relu'))\n",
    "    # Dropout\n",
    "    model.add(Dropout(0.4))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # D3 Dense Layer\n",
    "    model.add(Dense(1000))\n",
    "    model.add(Activation('relu'))\n",
    "    # Dropout\n",
    "    model.add(Dropout(0.4))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # Compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(Callback):\n",
    "\n",
    "    def __init__(self, valid_generator=None):\n",
    "        self.X_test, self.Y_test = None, None\n",
    "        self.valid_iterator = valid_generator\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.accuracy = []\n",
    "        self.f1_scores = []\n",
    "        self.recalls = []\n",
    "        self.precisions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.X_test, self.Y_test = self.valid_iterator.next()\n",
    "        pred = (np.asarray(self.model.predict(self.X_test))).round()\n",
    "        predicted_label = np.argmax(pred, axis=1)\n",
    "\n",
    "        target = self.Y_test\n",
    "        target_label = np.argmax(target, axis=1)\n",
    "\n",
    "        val_accuracy = nn.accuracy_score(target_label, predicted_label)\n",
    "        val_f1 = nn.f1_score(target_label, predicted_label, average='weighted')\n",
    "        val_recall = nn.recall_score(target_label, predicted_label, average='weighted')\n",
    "        val_precision = nn.precision_score(target_label, predicted_label, average='weighted')\n",
    "\n",
    "        self.accuracy.append(val_accuracy)\n",
    "        self.f1_scores.append(val_f1)\n",
    "        self.recalls.append(val_recall)\n",
    "        self.precisions.append(val_precision)\n",
    "\n",
    "        # print(confusion_matrix(target_label, predicted_label))\n",
    "        # print(classification_report(target_label, predicted_label))\n",
    "\n",
    "        print(\"Validation:\")\n",
    "        print(\"acc: {a:.2f} - f1-score: {f1:.2f} - prec: {p:.2f} - recall {r:.2f}\"\n",
    "              .format(a=val_accuracy, f1=val_f1, p=val_precision, r=val_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, metrics):\n",
    "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
    "    \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
    "    \n",
    "    # plot loss for each epoch\n",
    "    plt.figure(1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.3f'))+')'))\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plot accuracy for each epoch\n",
    "    plt.figure(2)\n",
    "    for l in acc_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.3f'))+')')   \n",
    "    \n",
    "    plt.plot(epochs, metrics.accuracy, 'r', label='Validation Accuracy (' + str(format(np.mean(metrics.accuracy),'.3f'))+')')\n",
    "    \n",
    "    plt.title('Accuracy Trend')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plot all mean performance metrics (accuracy, precision, recall and F1-score) for each epoch\n",
    "    plt.figure(3)\n",
    "    plt.plot(epochs, metrics.accuracy, 'r', label='Accuracy (' + str(format(np.mean(metrics.accuracy),'.3f'))+')')\n",
    "    plt.plot(epochs, metrics.precisions, 'g', label='Precision (' + str(format(np.mean(metrics.precisions),'.3f'))+')')\n",
    "    plt.plot(epochs, metrics.recalls, 'b', label='Recall (' + str(format(np.mean(metrics.recalls),'.3f'))+')')\n",
    "    plt.plot(epochs, metrics.f1_scores, 'y', label='F1-score (' + str(format(np.mean(metrics.f1_scores),'.3f'))+')')\n",
    "\n",
    "    plt.title('Validation set performances')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Performance metric')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(true_labels, predicted_labels, class_labels):\n",
    "    conf_matrix = nn.confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title('Confusion matrix')\n",
    "    sns.heatmap(conf_matrix.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "                xticklabels=sorted(class_labels), yticklabels=sorted(class_labels))\n",
    "    plt.xlabel('true label')\n",
    "    plt.ylabel('predicted label')\n",
    "    plt.draw()\n",
    "    plt.tight_layout()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_alexnet(train_dataframe, test_dataframe, class_labels, dir_path, batch_size, rand_seed, num_epochs, weights_filename, adv_preproc=False):\n",
    "    print('{:-<50}'.format(\"\"))\n",
    "    print(\"Image classification using AlexNet CNN classifier:\")\n",
    "\n",
    "    if class_labels is None:\n",
    "        print(\"Error: 'class_labels' cannot be None\")\n",
    "        return\n",
    "    \n",
    "    if train_dataframe is None:\n",
    "        print(\"Error: 'train_dataframe' cannot be None\")\n",
    "        return\n",
    "    \n",
    "    if test_dataframe is None:\n",
    "        print(\"Error: 'test_dataframe' cannot be None\")\n",
    "        return\n",
    "\n",
    "    num_samples_train = train_dataframe['ID'].count()\n",
    "    \n",
    "    if adv_preproc:\n",
    "        train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                           # featurewise_std_normalization=True,\n",
    "                                           rotation_range=180,\n",
    "                                           width_shift_range=0.2,\n",
    "                                           height_shift_range=0.2,\n",
    "                                           horizontal_flip=True,\n",
    "                                           zoom_range=0.15,\n",
    "                                           brightness_range=[0.8, 1.2],\n",
    "                                           data_format='channels_last')\n",
    "    else:\n",
    "        # Use ImageDataGenerator class to build an image generator for the training set.\n",
    "        # It will also rescale the pixel values between 0 and 1, that is by multiplying\n",
    "        # them by a factor of 1/255 since our original images consist in RGB coefficients\n",
    "        # in [0, 255], but such values would be too high for our model to process.\n",
    "        train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                           data_format='channels_last')\n",
    "        \n",
    "    train_iterator = train_datagen.flow_from_dataframe(train_dataframe,\n",
    "                                                       directory=dir_path,\n",
    "                                                       x_col='ID',\n",
    "                                                       y_col='CLASS',\n",
    "                                                       target_size=(227, 227),\n",
    "                                                       batch_size=batch_size,\n",
    "                                                       color_mode='rgb',\n",
    "                                                       class_mode='categorical',\n",
    "                                                       shuffle=True,\n",
    "                                                       seed=rand_seed)\n",
    "\n",
    "    # Use ImageDataGenerator class to build an image generator to be used for model validation.\n",
    "    # It will use the test set images as input data\n",
    "    valid_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                       data_format='channels_last')\n",
    "        \n",
    "    validation_iterator = valid_datagen.flow_from_dataframe(test_dataframe,\n",
    "                                                            directory=dir_path,\n",
    "                                                            x_col='ID',\n",
    "                                                            y_col='CLASS',\n",
    "                                                            target_size=(227, 227),\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            color_mode='rgb',\n",
    "                                                            class_mode='categorical',\n",
    "                                                            shuffle=True,\n",
    "                                                            seed=rand_seed)\n",
    "        \n",
    "    # Use ImageDataGenerator class to build an image generator to be used for\n",
    "    # performance evaluation.\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                      data_format='channels_last')\n",
    "\n",
    "    test_iterator = test_datagen.flow_from_dataframe(test_dataframe,\n",
    "                                                     directory=dir_path,\n",
    "                                                     x_col='ID',\n",
    "                                                     y_col='CLASS',\n",
    "                                                     target_size=(227, 227),\n",
    "                                                     batch_size=1,\n",
    "                                                     class_mode='categorical',\n",
    "                                                     shuffle=False)\n",
    "    # create a Metrics instance for validation\n",
    "    metrics = Metrics(validation_iterator)\n",
    "\n",
    "    # build AlexNet model\n",
    "    model = AlexNet(len(class_labels))\n",
    "\n",
    "    # print model summary\n",
    "    model.summary()\n",
    "\n",
    "    # train the AlexNet CNN\n",
    "    try:\n",
    "        history = model.fit_generator(train_iterator,\n",
    "                                      epochs=num_epochs,\n",
    "                                      steps_per_epoch=num_samples_train // batch_size,\n",
    "                                      verbose=1,\n",
    "                                      callbacks=[metrics])\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    model.save(weights_filename)\n",
    "    print('\\nModel weights saved successfully on file {name}\\n'.format(name=weights_filename))\n",
    "\n",
    "    # print and plot mean values of performance metrics related to the Validation set\n",
    "    print('*** VALIDATION SET PERFORMANCE EVALUATION ***')\n",
    "    print('Mean accuracy: {:.3f}'.format(np.mean(metrics.accuracy)))\n",
    "    print('Mean precision: {:.3f}'.format(np.mean(metrics.precisions)))\n",
    "    print('Mean recall: {:.3f}'.format(np.mean(metrics.recalls)))\n",
    "    print('Mean f1-score: {:.3f}'.format(np.mean(metrics.f1_scores)))\n",
    "    \n",
    "    plot_history(history, metrics)\n",
    "\n",
    "    # *** TEST SET PERFORMANCE EVALUATION ***\n",
    "    # get prediction on test data\n",
    "    y_pred = model.predict_generator(test_iterator, steps=len(test_iterator), verbose=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # compute and plot performance metrics values for each class\n",
    "    accuracy = nn.accuracy_score(test_iterator.classes, y_pred)\n",
    "    val_f1 = nn.f1_score(test_iterator.classes, y_pred, average='weighted')\n",
    "    val_recall = nn.recall_score(test_iterator.classes, y_pred, average='weighted')\n",
    "    val_precision = nn.precision_score(test_iterator.classes, y_pred, average='weighted')\n",
    "\n",
    "    print('*** TEST SET PERFORMANCE EVALUATION - AlexNet CNN ***')\n",
    "    print('Accuracy: {:.3f}'.format(accuracy))\n",
    "    print('F1-score: {:.3f}'.format(val_f1))\n",
    "    print('Recall: {:.3f}'.format(val_recall))\n",
    "    print('Precision: {:.3f}'.format(val_precision))\n",
    "\n",
    "    # print classification report and plot confusion matrix\n",
    "    print('\\nClassification Report')\n",
    "    print(nn.classification_report(test_iterator.classes, y_pred))\n",
    "\n",
    "    plot_confusion_matrix(test_iterator.classes, y_pred, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/Users/matthieu.kaeppelin/Documents/4-TélécomParis/1-Cours/IMA/IMA-205/Projet-IMA205/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Image classification using AlexNet CNN classifier:\n",
      "Found 15198 validated image filenames belonging to 8 classes.\n",
      "Found 3800 validated image filenames belonging to 8 classes.\n",
      "Found 3800 validated image filenames belonging to 8 classes.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_10 (Conv2D)          (None, 55, 55, 96)        34944     \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 55, 55, 96)        0         \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 27, 27, 96)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_16 (Ba  (None, 27, 27, 96)        384       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 17, 17, 256)       2973952   \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 17, 17, 256)       0         \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 8, 8, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_17 (Ba  (None, 8, 8, 256)         1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 6, 6, 384)         885120    \n",
      "                                                                 \n",
      " activation_20 (Activation)  (None, 6, 6, 384)         0         \n",
      "                                                                 \n",
      " batch_normalization_18 (Ba  (None, 6, 6, 384)         1536      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 4, 4, 384)         1327488   \n",
      "                                                                 \n",
      " activation_21 (Activation)  (None, 4, 4, 384)         0         \n",
      "                                                                 \n",
      " batch_normalization_19 (Ba  (None, 4, 4, 384)         1536      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 2, 2, 256)         884992    \n",
      "                                                                 \n",
      " activation_22 (Activation)  (None, 2, 2, 256)         0         \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (None, 1, 1, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_20 (Ba  (None, 1, 1, 256)         1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 4096)              1052672   \n",
      "                                                                 \n",
      " activation_23 (Activation)  (None, 4096)              0         \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " batch_normalization_21 (Ba  (None, 4096)              16384     \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " activation_24 (Activation)  (None, 4096)              0         \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " batch_normalization_22 (Ba  (None, 4096)              16384     \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1000)              4097000   \n",
      "                                                                 \n",
      " activation_25 (Activation)  (None, 1000)              0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 1000)              0         \n",
      "                                                                 \n",
      " batch_normalization_23 (Ba  (None, 1000)              4000      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 8)                 8008      \n",
      "                                                                 \n",
      " activation_26 (Activation)  (None, 8)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28087760 (107.15 MB)\n",
      "Trainable params: 28066624 (107.07 MB)\n",
      "Non-trainable params: 21136 (82.56 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zz/y6p01ncd2z9_ty6kfx1zk1mh0000gn/T/ipykernel_45961/3933080916.py:88: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(train_iterator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21/237 [=>............................] - ETA: 5:19 - loss: 2.7431 - accuracy: 0.3408INFO:tensorflow:Assets written to: weights_filename_Alex_Net/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: weights_filename_Alex_Net/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model weights saved successfully on file weights_filename_Alex_Net\n",
      "\n",
      "*** VALIDATION SET PERFORMANCE EVALUATION ***\n",
      "Mean accuracy: nan\n",
      "Mean precision: nan\n",
      "Mean recall: nan\n",
      "Mean f1-score: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthieu.kaeppelin/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/matthieu.kaeppelin/anaconda3/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'history' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run_alexnet(df_train, df_val, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCLASS1\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCLASS2\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCLASS3\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCLASS4\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCLASS5\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCLASS6\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCLASS7\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCLASS8\u001b[39m\u001b[38;5;124m'\u001b[39m], dir_path, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights_filename_Alex_Net\u001b[39m\u001b[38;5;124m'\u001b[39m, adv_preproc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[43], line 106\u001b[0m, in \u001b[0;36mrun_alexnet\u001b[0;34m(train_dataframe, test_dataframe, class_labels, dir_path, batch_size, rand_seed, num_epochs, weights_filename, adv_preproc)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean recall: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(np\u001b[38;5;241m.\u001b[39mmean(metrics\u001b[38;5;241m.\u001b[39mrecalls)))\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean f1-score: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(np\u001b[38;5;241m.\u001b[39mmean(metrics\u001b[38;5;241m.\u001b[39mf1_scores)))\n\u001b[0;32m--> 106\u001b[0m plot_history(history, metrics)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# *** TEST SET PERFORMANCE EVALUATION ***\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# get prediction on test data\u001b[39;00m\n\u001b[1;32m    110\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_generator(test_iterator, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_iterator), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'history' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "run_alexnet(df_train, df_val, ['CLASS1','CLASS2','CLASS3','CLASS4','CLASS5','CLASS6','CLASS7','CLASS8'], dir_path, 64, 0, 64, 'weights_filename_Alex_Net', adv_preproc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_vanilla_backprop(input_image, model, target_class):\n",
    "    # Charger le modèle\n",
    "    model = model.eval() # passer en mode évaluation\n",
    "\n",
    "    # Définir l'image d'entrée\n",
    "\n",
    "    #[batch_size, num_channels, height, width]\n",
    "    input_tensor = torch.unsqueeze(torch.tensor(input_image), 0).permute(0, 3, 1, 2).float()\n",
    "\n",
    "    # Définir la classe cible\n",
    "    target_class = 1 # la classe cible\n",
    "\n",
    "    # Créer une instance de VanillaBackprop\n",
    "    vanilla_backprop = vb.VanillaBackprop(model)\n",
    "\n",
    "    # Calculer les gradients avec Vanilla Backpropagation\n",
    "    gradients = vanilla_backprop.generate_gradients(input_tensor, target_class)\n",
    "\n",
    "    # Convertir les gradients en masque de pertinence\n",
    "    masque = np.abs(gradients)\n",
    "    masque = masque / np.max(masque) # normaliser entre 0 et 1\n",
    "\n",
    "    # Afficher le masque de pertinence\n",
    "    heatmap = masque.mean(axis=0)\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "def plot_vanilla_backprop(liste_images,model):\n",
    "    target_classes = [0,1,2,3,4,5,6,7]\n",
    "    # Create subplots for heatmaps\n",
    "    fig, axs = plt.subplots(len(liste_images), len(target_classes), figsize=(15, 10))\n",
    "\n",
    "    # Iterate over images\n",
    "    for i, image in enumerate(liste_images):\n",
    "        # Iterate over target classes\n",
    "        for j, target_class in enumerate(target_classes):\n",
    "            # Compute heatmap for the image and target class\n",
    "            heatmap = image_vanilla_backprop(image, model, target_class)\n",
    "            \n",
    "            # Display heatmap\n",
    "            axs[i, j].imshow(heatmap, cmap='hot')\n",
    "            axs[i, j].set_title(f'Target Class: {target_class}')\n",
    "            axs[i, j].axis('off')\n",
    "\n",
    "    # Display target classes in a row\n",
    "    fig, ax = plt.subplots(1, len(target_classes), figsize=(15, 2))\n",
    "    for j, target_class in enumerate(target_classes):\n",
    "        ax[j].imshow(target_class_image)  # Replace with your own target class image\n",
    "        ax[j].set_title(f'Target Class: {target_class}')\n",
    "        ax[j].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vanilla_backprop(X_train[:5],model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/colormap.cpp:736: error: (-5:Bad argument) cv::ColorMap only supports source images of type CV_8UC1 or CV_8UC3 in function 'operator()'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m grayscale_cam \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(grayscale_cam)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39muint8)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Visualiser la carte de chaleur de grad-CAM superposée à l'image d'origine\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m visualization \u001b[38;5;241m=\u001b[39m visualize_cam(img_tensor, grayscale_cam)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Afficher l'image\u001b[39;00m\n\u001b[1;32m     43\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(visualization\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gradcam/utils.py:26\u001b[0m, in \u001b[0;36mvisualize_cam\u001b[0;34m(mask, img, alpha)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make heatmap from mask and synthesize GradCAM result image using heatmap and img.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    mask (torch.tensor): mask shape of (1, 1, H, W) and each element has value in range [0, 1]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    result (torch.tensor): synthesized GradCAM result of same shape with heatmap.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m mask\u001b[38;5;241m.\u001b[39msqueeze())\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39muint8)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 26\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mapplyColorMap(heatmap, cv2\u001b[38;5;241m.\u001b[39mCOLORMAP_JET)\n\u001b[1;32m     27\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(heatmap)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m     28\u001b[0m b, g, r \u001b[38;5;241m=\u001b[39m heatmap\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.1) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/colormap.cpp:736: error: (-5:Bad argument) cv::ColorMap only supports source images of type CV_8UC1 or CV_8UC3 in function 'operator()'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from gradcam import GradCAM\n",
    "from gradcam.utils import visualize_cam\n",
    "from PIL import Image\n",
    "\n",
    "# Charger une image à visualiser\n",
    "img = Image.open('output_masks_train_set_1/ISIC_0000000.jpg_pred_mask.png')\n",
    "\n",
    "# Transformer l'image en tenseur et l'ajouter à un lot\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Modifiez les moyennes et écarts-types en fonction de votre jeu de données\n",
    "])\n",
    "img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "# Définir la classe cible pour la visualisation\n",
    "target_class = 3  # Remplacez par la classe cible souhaitée\n",
    "\n",
    "# Créer un objet GradCAM pour la dernière couche de convolution du modèle\n",
    "cam = GradCAM(model,model[-5])\n",
    "\n",
    "# Calculer la carte de chaleur de grad-CAM pour la classe cible\n",
    "with torch.no_grad():\n",
    "    output = model(img_tensor)\n",
    "    target_logits = output[:, target_class]\n",
    "    target_logits = target_logits.long()\n",
    "grayscale_cam = cam(img_tensor,target_logits)\n",
    "grayscale_cam = grayscale_cam.squeeze().type(torch.uint8)\n",
    "# Visualiser la carte de chaleur de grad-CAM superposée à l'image d'origine\n",
    "visualization = visualize_cam(img_tensor, grayscale_cam)\n",
    "\n",
    "# Afficher l'image\n",
    "plt.imshow(visualization.cpu().numpy().transpose(1, 2, 0))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation de la detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for predicting the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLes cellules en cours d’exécution avec Python 3.9.18 nécessitent le package ipykernel.\n",
      "\u001b[1;31mExécutez la commande suivante pour installer 'ipykernel' dans l’environnement Python. \n",
      "\u001b[1;31mCommande : '/usr/bin/python3.9 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "X_test = glob.glob('output_masks_test_set/*.png')\n",
    "X_test_names = [os.path.basename(x).split('.jpg_pred_mask.png')[0] for x in X_test]\n",
    "\n",
    "X_test = [io.imread(x) for x in X_test]\n",
    "X_test = np.array(X_test)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLes cellules en cours d’exécution avec Python 3.9.18 nécessitent le package ipykernel.\n",
      "\u001b[1;31mExécutez la commande suivante pour installer 'ipykernel' dans l’environnement Python. \n",
      "\u001b[1;31mCommande : '/usr/bin/python3.9 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "predicted_classes = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLes cellules en cours d’exécution avec Python 3.9.18 nécessitent le package ipykernel.\n",
      "\u001b[1;31mExécutez la commande suivante pour installer 'ipykernel' dans l’environnement Python. \n",
      "\u001b[1;31mCommande : '/usr/bin/python3.9 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print(predicted_classes.shape)\n",
    "print(predicted_classes)\n",
    "\n",
    "predicted_classes_final = [np.argmax(predicted_classes[i]) for i in range(0, len(predicted_classes))]\n",
    "\n",
    "print(predicted_classes_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ecrire les résultats predicted class dans un fichier csv de 2 colonnes avec 1 colonne avec le nom correspondant à la classe et 1 colonne correspondant à la classe prédite. Le séparateur est une virgule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLes cellules en cours d’exécution avec Python 3.9.18 nécessitent le package ipykernel.\n",
      "\u001b[1;31mExécutez la commande suivante pour installer 'ipykernel' dans l’environnement Python. \n",
      "\u001b[1;31mCommande : '/usr/bin/python3.9 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Open the CSV file for writing\n",
    "with open('SampleSubmission2.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write the header row\n",
    "    writer.writerow(['ID', 'CLASS'])\n",
    "\n",
    "    # Write the predicted classes and image filenames to the CSV file\n",
    "    for i in range(len(predicted_classes)):\n",
    "        writer.writerow([X_test_names[i], predicted_classes_final[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLes cellules en cours d’exécution avec Python 3.9.18 nécessitent le package ipykernel.\n",
      "\u001b[1;31mExécutez la commande suivante pour installer 'ipykernel' dans l’environnement Python. \n",
      "\u001b[1;31mCommande : '/usr/bin/python3.9 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
